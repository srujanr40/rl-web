
[{"content":"","date":"21 June 2024","externalUrl":null,"permalink":"/","section":"","summary":"","title":"","type":"page"},{"content":"","date":"21 June 2024","externalUrl":null,"permalink":"/posts/","section":"","summary":"","title":"","type":"posts"},{"content":"Here is some content. This is all in markdown.\nhere there everywhere Dictionary dictionary definition ","date":"21 June 2024","externalUrl":null,"permalink":"/about/","section":"","summary":"Here is some content.","title":"About","type":"page"},{"content":"","date":"21 June 2024","externalUrl":null,"permalink":"/tags/article/","section":"Tags","summary":"","title":"Article","type":"tags"},{"content":" Introduction to the problem # The cartpole problem involves a movable cart on a track with a pole attached to its center. The only actions the agent can perform are to move the cart a step to the left, or a step to the right. The agent can fully observe the state of the environment at any given time step. The pole on the cart can rotate freely and is weighed down by gravity.\nAn environment for this problem is generously available in the OpenAI Gym library through the CartPole-v1 environment. This library provides all the necessary tools to set up the problem and interact with the environment, allowing us to focus on a solution of the problem instead of the details of the environment. It also allows us to visualize the cartpole, and easily observe how the actions of the agent affect the positions of the cart and pole.\nDemo of the problem with the agent taking random actions\n{{include image here}}\nThe Goal # The goal of the problem is to get the agent to keep the pole on the cart upright. To do so, the agent needs to move the cart to the left or right just the right amount at each time step such that the movement of the cart balances the movement of the pole against gravity.\nWhat can the agent observe? # The agent can observe the cart position on the x-axis, the cart velocity, the pole angle in radians, and the pole angular velocity.\nApproach # Q-Learning # For the agent to know what upright is, it must be given rewards for achieving the required state and be penalized for not achieving the required state. In the cartpole environment, the agent gains a +1 reward for every step it take where the pole is maintained upright, and no reward if the pole fails to be upright. By encouraging the agent to take actions that maximize rewards, we can encourage actions that keep the pole upright.\nTo teach the agent to keep the pole upright, we can use Q-Learning utilizing Bellmans\u0026rsquo; equations. Q-Learning is an algorithm that learns from the environment by updating the Q-Values of the state-action pairs. By repeatedly training the agent on the same environment, the agent can take actions and then observe the outcome of the action and the reward it obtains from said action. We can use this to build a state-action pair table with values that correspond to the probability of maximizing rewards based on the action taken.\nBellman\u0026rsquo;s Optimality Equation for Q-values $$ Q[s_t, a_t] = r[s_t, a_t] + \\gamma \\max_a Q[s_{t+1}, a] $$\nQ-Learning Algorithm to update Q-values for state-action pairs $$ Q[s, a] \\leftarrow Q[s,a] + \\alpha((r + \\gamma \\max_{a\u0026rsquo;}Q[s\u0026rsquo;,a\u0026rsquo;]) - Q[s,a]) $$\nEpsilon Greedy Policy # Q-learning can create a state-action pair table, but does not tell the agent what action to take at what state.\nTo be able to exploit this data, we can use an epsilon-greedy policy. We want the agent to take more random steps at the beginning of training as it encourages exploration, allowing the agent to explore all possible actions and paths that may lead to the highest overall rewards rather than pigeonholing it into one series of actions. However, with enough iterations and enough exploration, we would like the agent to get greedier and take more action where the Q-value is higher so it gets better at refining the optimal series of actions for maximum rewards.\nWith the epsilon-greedy policy, the agent has a chance of taking a random action and a chance of taking the greedy action. The greedy action is the action that maximizes the Q-value. We can start with a low epislon value to encourage the agent to explore. By increasing the value of epsilon over time, we can get the agent to exploit instead of explore.\nEpsilon-Greedy Policy\nheading 3 # ","date":"21 June 2024","externalUrl":null,"permalink":"/posts/cart-pole/","section":"","summary":"Introduction to the problem # The cartpole problem involves a movable cart on a track with a pole attached to its center.","title":"Cartpole Problem","type":"posts"},{"content":"Here is something about the mountain car problem\nheading 2 # heading 3 # ","date":"21 June 2024","externalUrl":null,"permalink":"/posts/mountain-car/","section":"","summary":"Here is something about the mountain car problem","title":"Mountain Car Problem","type":"posts"},{"content":"","date":"21 June 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]